{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到 kiba_matrix_triplets_processed.csv 文件中。\n"
     ]
    }
   ],
   "source": [
    "# Add an index value to each row\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_path = \"kiba_matrix_triplets.csv\"  # Input file path\n",
    "output_path = \"kiba_matrix_triplets_processed.csv\"  # Output file path\n",
    "\n",
    "# Read data\n",
    "data = pd.read_csv(input_path, header=None)\n",
    "data.columns = [\"DrugID\", \"ProteinID\", \"Affinity\"]  # Add column names\n",
    "\n",
    "# Fill NaN values with -5\n",
    "data.fillna(-5, inplace=True)\n",
    "\n",
    "# Add a new column containing \"# and the corresponding row index\"\n",
    "data[\"NewColumn\"] = data.index.map(lambda x: f\"#{x}\")\n",
    "\n",
    "# Save the result to a new file\n",
    "data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"The processed data has been saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理后的数据已保存到 kiba_matrix_triplets_filtered.csv 文件中。\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where the Affinity value is -5\n",
    "import pandas as pd\n",
    "\n",
    "# File paths\n",
    "input_path = \"kiba_matrix_triplets_processed.csv\"  # Input file path\n",
    "output_path = \"kiba_matrix_triplets_filtered.csv\"  # Output file path\n",
    "\n",
    "# Read data\n",
    "data = pd.read_csv(input_path)\n",
    "\n",
    "# Remove rows where the value in the Affinity column is -5\n",
    "filtered_data = data[data[\"Affinity\"] != -5]\n",
    "\n",
    "# Save the remaining data to a new file\n",
    "filtered_data.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"The processed data has been saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "# Read data\n",
    "file_path = \"kiba_matrix_triplets_filtered.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Initialize 5-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Group by DrugID\n",
    "grouped = data.groupby(\"DrugID\")\n",
    "\n",
    "# Cross-validation output directory and file templates\n",
    "output_dir = \"./cross_validation_sets/\"\n",
    "train_file_template = output_dir + \"train_indices_fold_{}.csv\"\n",
    "test_file_template = output_dir + \"test_indices_fold_{}.csv\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# Iterate over each fold in cross-validation\n",
    "for fold, (train_idx, test_idx) in enumerate(kf.split(data)):\n",
    "    train_file_path = train_file_template.format(fold + 1)\n",
    "    test_file_path = test_file_template.format(fold + 1)\n",
    "    \n",
    "    # Save training set\n",
    "    train_indices = []\n",
    "    for drug_id, group in grouped:\n",
    "        train_indices += group.index.intersection(train_idx).tolist()\n",
    "    train_data = data.loc[train_indices, [\"NewColumn\"]]\n",
    "    train_data[\"Index\"] = train_data.index\n",
    "    train_data = train_data[[\"Index\", \"NewColumn\"]]\n",
    "    train_data.to_csv(train_file_path, index=False)\n",
    "    \n",
    "    # Save testing set\n",
    "    test_indices = []\n",
    "    for drug_id, group in grouped:\n",
    "        test_indices += group.index.intersection(test_idx).tolist()\n",
    "    test_data = data.loc[test_indices, [\"NewColumn\"]]\n",
    "    test_data[\"Index\"] = test_data.index\n",
    "    test_data = test_data[[\"Index\", \"NewColumn\"]]\n",
    "    test_data.to_csv(test_file_path, index=False)\n",
    "    \n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "    print(f\"  Training set saved to {train_file_path}\")\n",
    "    print(f\"  Testing set saved to {test_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "处理完成: train_test_fold/train_indices_fold_1.csv\n",
      "处理完成: train_test_fold/test_indices_fold_1.csv\n",
      "处理完成: train_test_fold/train_indices_fold_2.csv\n",
      "处理完成: train_test_fold/test_indices_fold_2.csv\n",
      "处理完成: train_test_fold/train_indices_fold_3.csv\n",
      "处理完成: train_test_fold/test_indices_fold_3.csv\n",
      "处理完成: train_test_fold/train_indices_fold_4.csv\n",
      "处理完成: train_test_fold/test_indices_fold_4.csv\n",
      "处理完成: train_test_fold/train_indices_fold_5.csv\n",
      "处理完成: train_test_fold/test_indices_fold_5.csv\n"
     ]
    }
   ],
   "source": [
    "# Batch process all files in the cross_validation_sets/ folder:\n",
    "# Extract the number after '#' in the NewColumn column, sort in ascending order,\n",
    "# and output to the corresponding file.\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder paths\n",
    "input_folder = \"cross_validation_sets/\"\n",
    "output_folder = \"train_test_fold/\"\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Get all file names\n",
    "file_list = [f for f in os.listdir(input_folder) if f.endswith(\".csv\")]\n",
    "\n",
    "# Batch process files\n",
    "for file_name in file_list:\n",
    "    input_path = os.path.join(input_folder, file_name)\n",
    "    output_path = os.path.join(output_folder, file_name)\n",
    "    \n",
    "    # Read the file\n",
    "    data = pd.read_csv(input_path)\n",
    "    \n",
    "    # Extract the numeric part after '#' in the NewColumn column\n",
    "    data['Processed'] = data['NewColumn'].str.extract(r'#(\\d+)').astype(int)\n",
    "    \n",
    "    # Sort in ascending order\n",
    "    sorted_data = data['Processed'].sort_values().reset_index(drop=True)\n",
    "    \n",
    "    # Output to file\n",
    "    sorted_data.to_csv(output_path, index=False, header=False)\n",
    "    \n",
    "    print(f\"Processing completed: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
